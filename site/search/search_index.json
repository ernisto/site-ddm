{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fazendo seu pr\u00f3prio parser","text":"<p>Parsers s\u00e3o programas respons\u00e1veis por transformar um texto (que seguir uma gramatica) em uma estrutura de dado mais f\u00e1cil de trabalhar. S\u00e3o comuns em ferramentas de linguagem de programa\u00e7\u00e3o, como compiladores, interpretadores, e at\u00e9 mais comumente para transformar em um hash map, ou dicion\u00e1rio manipul\u00e1vel o seu JSON, YAML, XML, etc. Aqui ser\u00e1 mostrado como fazer um parser tipicamente de uma linguagem de programa\u00e7\u00e3o, usando a linguagem Luau, uma linguagem de programa\u00e7\u00e3o de scripting simples, r\u00e1pida e tipada, derivada da linguagem Lua.</p>"},{"location":"#abstracoes-divisoes-do-programa","title":"Abstra\u00e7\u00f5es (divis\u00f5es do programa)","text":"<p>Parsers de linguagens de programa\u00e7\u00e3o, diferente de arquivos de configura\u00e7\u00e3o simples, s\u00e3o normalmente divididas em algumas etapas, tanto por desempenho, quanto para fornecer mais recursos para os compiladores e interpretadores.</p> <ol> <li>O Lexer, que ser\u00e1 respons\u00e1vel por ler todo o texto, e transformar v\u00e1rios pedacinhos desses textos em Tokens (lexing), que geralmente vai ser um pequeno objeto que guarda seu tipo, e opcionalmente vai guardar algumas outras informa\u00e7\u00f5es \u00fateis, como o texto cru do Token, e posi\u00e7\u00e3o onde o Token foi encontrado.<ul> <li>Tokens s\u00e3o pequenos peda\u00e7os do texto, eles s\u00e3o usados pelo Parser para discernir pequenos peda\u00e7os de texto mais eficientemente, inv\u00e9s de sempre fazer lexing em contextos possivelmente errados, o que resultara no lexing sendo feito v\u00e1rias vezes para o mesmo Token.</li> <li>Alguns tipos de Tokens bem comuns em uma linguagem de programa\u00e7\u00e3o seriam<ul> <li>palavras (como if, ou nome das vari\u00e1veis).</li> <li>N\u00fameros (como 10, 1.5, ou 3e+2).</li> <li>Strings (como \"Ol\u00e1 Mundo\").</li> </ul> </li> </ul> </li> <li>O Token Stream (corrente/fluxo de Tokens), que ser\u00e1 a parte do programa respons\u00e1vel por dar o Token atual, avan\u00e7ar para o pr\u00f3ximo, e ocasionalmente voltar para um anterior. \u00c9 importante ter um fluxo sequencial dos Tokens, porque vamos descartando os tokens j\u00e1 usados, facilitando o trabalho com os tokens restantes.</li> <li>O pr\u00f3prio Parser em si, que ser\u00e1 quem vai ler toda a cadeia de Tokens para a pr\u00f3pria estrutura de dados mais conveniente para voc\u00ea, se for um parser de JSON voc\u00ea vai querer transformar em uma Map, no caso de linguagens de programa\u00e7\u00e3o, vamos transformar em uma AST (Abstract Syntax Tree).<ul> <li>AST como uma \u00e1rvore que tem v\u00e1rios galhos emergidos de outros galhos, \u00e1rvores na programa\u00e7\u00e3o tamb\u00e9m tem esses \"galhos\", por\u00e9m s\u00e3o chamados de Nodes (n\u00f3s), onde cada Node pode se referir a alguns outros Nodes, cada tipo de node vai representar um conjunto de tokens, com esse conjunto voc\u00ea vai aos poucos juntando, e obtendo informa\u00e7\u00f5es mais detalhadas do c\u00f3digo.</li> <li>Alguns tipos de Nodes t\u00edpicos em uma linguagem de programa\u00e7\u00e3o s\u00e3o<ul> <li>Node de blocos de c\u00f3digo, onde guarda v\u00e1rios outros Nodes de Statements.</li> <li>Nodes de Statements (instru\u00e7\u00f5es), como um.<ul> <li>Node para defini\u00e7\u00e3o de vari\u00e1vel, onde guardara o nome da vari\u00e1vel, e um Node de Expression para representar o valor inicial dela.</li> <li>Node para um if, que guardaria v\u00e1rios n\u00f3s possibilidades, onde cada possibilidade teria um Node de Expression para representar a condi\u00e7\u00e3o, e outro de bloco, para as instru\u00e7\u00f5es executadas caso a condi\u00e7\u00e3o seja verdadeira.</li> </ul> </li> <li>Nodes de Expressions (express\u00f5es), que representam valores primitivos, ou opera\u00e7\u00f5es de outras express\u00f5es.</li> </ul> </li> </ul> </li> </ol>"},{"location":"#mao-na-massa","title":"M\u00e3o na massa","text":"<p>Primeiro vamos come\u00e7ar definindo os tipos de tokens <pre><code>type pos = {\n\u00a0 \u00a0 x: number, -- coluna dentro da linha atual\n\u00a0 \u00a0 y: number, \u00a0-- linha do c\u00f3digo fonte\n\u00a0 \u00a0 z: number \u00a0-- coluna absoluta, sendo somada com a coluna das outras linhas\n}\n\ntype Token = NumberToken | StringToken | WordToken | CharToken\ntype baseToken = { -- tipo base para os outros tipos de tokens, com dados que todos terao\n\u00a0 \u00a0 start: pos,\n\u00a0 \u00a0 final: pos,\n\u00a0 \u00a0 trivia: string \u00a0-- espa\u00e7os em branco ap\u00f3s o token\n}\n\ntype StringToken = baseToken &amp; { kind: 'string', content: string }\ntype NumberToken = baseToken &amp; { kind: 'number', value: number }\ntype WordToken = baseToken &amp; { kind: 'word', text: string }\ntype CharToken = baseToken &amp; { kind: 'char', char: string }\n</code></pre> E ent\u00e3o vamos come\u00e7ar com o lexer, como um tutorial introdut\u00f3rio, vai ser uma implementa\u00e7\u00e3o simples, por\u00e9m lerda (ainda mais por estarmos usando uma linguagem de alto n\u00edvel) por\u00e9m vai ser suficiente para entender os fundamentos. <pre><code>-- context\nlocal x = 1\nlocal y = 1\nlocal z = 1    -- posicao inicial da string que estamos lendo\nlocal pos = { x = x, y = y, z = z }\nlocal tokens = {}\n\n-- redefine estados globais do modulo\n-- pode ocassionar bugs se fizer isso, antes de terminar de usar os estados anteriores\nlocal function lex(sourceCode: string) -&gt; {Token}\n\n    x = 1\n    y = 1\n    z = 1    -- posicao inicial da string que estamos lendo\n    pos = { x = x, y = y, z = z }\n    tokens = {}\nend\n\n-- futuramente podemos incluir gramatica de coment\u00e1rios aqui\nlocal function scanTrivia(): string\n\n    local trivias = {}\n\n    local _,final, trivia = string.find(sourceCode, \"^([ \\t]*)\", z)\n    if final then table.insert(trivias, trivia); z = final+1 end\n\n    while string.sub(sourceCode, z, z) == '\\n' do\n        x = 1\n        y += 1\n        z += 1\n        _,final, trivia = string.find(sourceCode, \"^([ \\t]*)\", z)\n        if final then table.insert(trivias, trivia); z = final+1 end\n    end\n\n    pos = { x = x, y = y, z = z }\n    return table.concat(trivias)\nend\n\n-- normalmente\nlocal function scanNumber(): NumberToken?\n\n    local start = pos\n\n    local _,final, decimal = string.find(sourceCode, \"^(%d+)\", z)\n    if decimal then z = final+1 end\n\n    local _,final, fractional = string.find(sourceCode, \"^%.(%d+)\", z)\n    if fractional then z = final+1 end\n\n    if not decimal and not fractional then return end  -- aqui ja desconsideramos esse token como um n\u00famero\n\n    local final = { x = x + (z - start.z - 1), y = y, z = final }\n    return { kind = 'number',\n        start = start,\n        final = final,\n        trivia = scanTrivia(),\n\n        decimal = decimal,\n        fractional = fractional,\n        exponent = exponent,\n        expSign = sign,\n    }\nend\nlocal function scanString(): StringToken?\n\n    local start = pos\n\n    local _,final, content = string.find(sourceCode, \"^(%b\\\"\\\")\")\n    if not final then return end\n    z = final - 1\n\n    local final = { x = x + (z - start.z - 1), y = y, z = final }\n    return { kind = 'string'\n        start = start,\n        final = final,\n        trivia = scanTrivia(),\n        content = content\n    }\nend\nlocal function scanWord(): WordToken?\n\n    local start = pos\n    local _,final, word = string.find(sourceCode, \"^(%w+)\")\n    if not final then return end\n\n    z = final - 1\n\n    local final = { x = x + (z - start.z - 1), y = y, z = final }\n    return { kind = 'word',\n        start = start,\n        final = final,\n        trivia = scanTrivia(),\n        word = word,\n    }\nend\nlocal function scanChar(): Token\n\n    local start = pos\n    local _,final, char = string.find(sourceCode, \"^(.)\")\n    z += 1\n\n    local final = { x = x + 1, y = y, z = final }\n    return { kind = 'char',\n        start = start,\n        final = final,\n        trivia = scanTrivia(),\n        char = char,\n    }\nend\nlocal function scanToken()\n    return scanNumber()\n        or scanString()\n        or scanWord()\n        or scanChar()\nend\n</code></pre> Ap\u00f3s ter feito o Lexer, vamos tokenizar tudo de uma vez, e expor alguns m\u00e9todos para consumir os tokens desde o in\u00edcio, descartando os anteriores <pre><code>local tokens: {Token}\nlocal cursor: number\nlocal token: Token\n\nlocal function tokenizeAll(_sourceCode: string)\n\n\u00a0 \u00a0 lex(_sourceCode)\n\u00a0 \u00a0 tokens = {}\n\u00a0 \u00a0 repeat\n\u00a0 \u00a0 \u00a0 \u00a0 local token = scanToken()\n\u00a0 \u00a0 \u00a0 \u00a0 if token then table.insert(tokens, token) end\n\n\u00a0 \u00a0 until not token\n\n\u00a0 \u00a0 cursor = 1\n\u00a0 \u00a0 token = tokens[cursor]\nend\n\nlocal function consume(): Token\n\n\u00a0 \u00a0 local last = token\n\u00a0 \u00a0 cursor += 1\n\u00a0 \u00a0 token = tokens[cursor]\n\u00a0 \u00a0 return last\nend\n\n-- uma das raz\u00f5es para tokenizar tudo primeiro, \u00e9 para n\u00e3o ter que re-tokenizar o mesmo\n-- token Word por n\u00e3o atender a diferentes exigencias do parametro 'specific' que v\u00e3o aparecer conforme o parsing de um arquivo inteiro\nlocal function consumeWord(specific: string?): WordToken?\n\u00a0 \u00a0 return if token and token.kind == 'word' and (not specific or token.text == specific)\n\u00a0 \u00a0 \u00a0 \u00a0 then consume() :: any else nil\nend\nlocal function consumeChar(specific: string?): CharToken?\n\u00a0 \u00a0 return if token and token.kind == 'char' and (not specific or token.char == specific)\n\u00a0 \u00a0 \u00a0 \u00a0 then consume() :: any else nil\nend\nlocal function consumeNum(): NumberToken?\n\u00a0 \u00a0 return if token and token.kind == 'number' then consume() :: any else nil\nend\nlocal function consumeStr(): StringToken?\n\u00a0 \u00a0 return if token and token.kind == 'string' then consume() :: any else nil\nend\n</code></pre> Ap\u00f3s ter feito o Lexer, vamos escrever o Token Stream, que usar\u00e1 o Lexer para gerar uma AST do c\u00f3digo recebido, usando os Tokens definidos previamente. vamos definir os tipos de Nodes(n\u00f3s) que teremos na nossa gram\u00e1tica <pre><code>type block = { kind: 'block', stats: {stat} }\ntype stat = assignment | callment | if_stat\ntype assignment = { kind: 'assignment', targetName: string, value: expr }\ntype callment = { kind: 'callment', func: expr, args: {expr} }\ntype if_stat = { kind: 'if', cond: expr, then_body: block, else_body: block? }\n\ntype expr = str_expr | num_expr | read_expr | biop_expr\ntype str_expr = { kind: 'str', str: StringToken }\ntype num_expr = { kind: 'num', num: NumberToken }\ntype read_expr = { kind: 'read', name: WordToken }\ntype biop_expr = { kind: 'biop', op: string, right: expr, left: expr }\n</code></pre></p> <p>E ent\u00e3o, vamos implementar o parser de fato. Vale notar que existe v\u00e1rias estrat\u00e9gias de implementa\u00e7\u00e3o, mas o parser implementado aqui \u00e9 um parser LL (Left-to-right Left-must), descendente e recursivo ser combinar pequenas fun\u00e7\u00f5es em fun\u00e7\u00f5es maiores. Esse tipo de estrat\u00e9gia \u00e9 a forma mais simples, e flex\u00edvel de se implementar. <pre><code>local parse_body: () -&gt; block?\n\nlocal function token_to_str(token: Token)\n\u00a0 \u00a0 return if token.kind == 'word' then token.text\n\u00a0 \u00a0 \u00a0 \u00a0 elseif token.kind == 'char' then token.char\n\u00a0 \u00a0 \u00a0 \u00a0 elseif token.kind == 'string' then token.content\n\u00a0 \u00a0 \u00a0 \u00a0 else tostring(token.value)\nend\nlocal function parsing_error(message: string): never\n\u00a0 \u00a0 error(`erro em {token.start.y}:{token.start.x}:\\\n\u00a0 \u00a0 \u00a0 \u00a0 {message} (recebido: '{token_to_str(token)}')`)\nend\n\n-- expr\n-- aqui \u00e9 importante a configura\u00e7\u00e3o de precedencia e associatividade de cada operador\nlocal LEVEL = { \u00a0 \u00a0 -- todos os operadores: left-associative: ex: ((1 + 2) + 3) + 4\n\u00a0 \u00a0 ['&lt;'] = 1,\n\u00a0 \u00a0 ['&lt;='] = 1,\n\u00a0 \u00a0 ['&gt;'] = 1,\n\u00a0 \u00a0 ['&gt;='] = 1,\n\u00a0 \u00a0 ['=='] = 1,\n\u00a0 \u00a0 ['!='] = 1,\n\n\u00a0 \u00a0 ['+'] = 2,\n\u00a0 \u00a0 ['-'] = 2,\n\n\u00a0 \u00a0 ['*'] = 3,\n\u00a0 \u00a0 ['/'] = 3,\n\u00a0 \u00a0 ['%'] = 3,\n\n\u00a0 \u00a0 ['^'] = 4, -- exceto esse: right-associative, ex: 2^(3^(4^5))\n}\n\nlocal function parse_atom(): expr?\n\n\u00a0 \u00a0 local str = consumeStr()\n\u00a0 \u00a0 if str then return { kind = 'str', str = str } end\n\n\u00a0 \u00a0 local num = consumeNum()\n\u00a0 \u00a0 if num then return { kind = 'num', num = num } end\n\n\u00a0 \u00a0 local name = consumeWord()\n\u00a0 \u00a0 if name then return { kind = 'read', name = name } end\n\n\u00a0 \u00a0 return\nend\n\nlocal function parse_expr(_left: expr?, _currentLevel: number?): expr?\n\n\u00a0 \u00a0 local left = _left or parse_atom()\n\u00a0 \u00a0 if not left then return end\n\n\u00a0 \u00a0 local currentLevel = _currentLevel or 0\n\u00a0 \u00a0 repeat\n\u00a0 \u00a0 \u00a0 \u00a0 local op = if token and token.kind == 'char' then token else nil\n\u00a0 \u00a0 \u00a0 \u00a0 if not op then break end\n\u00a0 \u00a0 \u00a0 \u00a0 local op = op.char\n\n\u00a0 \u00a0 \u00a0 \u00a0 local level = LEVEL[op]\n\u00a0 \u00a0 \u00a0 \u00a0 if not level then break end -- volta o cursor, como se estivesse devolvendo o token consumido\n\n\u00a0 \u00a0 \u00a0 \u00a0 if level &lt;= currentLevel then break end\n\u00a0 \u00a0 \u00a0 \u00a0 consume()\n\n\u00a0 \u00a0 \u00a0 \u00a0 local subParseLevel = if op == '^' then level-1 else level \u00a0-- por causa que esse operador em especifico \u00e9 right-associative\n\u00a0 \u00a0 \u00a0 \u00a0 local right = parse_expr(nil, subParseLevel)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 or parsing_error(`expressao esperada`)\n\n\u00a0 \u00a0 \u00a0 \u00a0 left = { kind = 'biop', op = op, left = left, right = right }\n\n\u00a0 \u00a0 until false\n\u00a0 \u00a0 return left\nend\n\n-- stat\nlocal function parse_assignment()\n\n\u00a0 \u00a0 if not consumeWord('set') then return end\n\u00a0 \u00a0 local varName = consumeWord()\n\u00a0 \u00a0 local _1 = consumeWord('to') or parsing_error(`token 'to' esperado`)\n\u00a0 \u00a0 local value = parse_expr()\n\n\u00a0 \u00a0 return {\n\u00a0 \u00a0 \u00a0 \u00a0 kind = 'assignment',\n\u00a0 \u00a0 \u00a0 \u00a0 targetName = varName,\n\u00a0 \u00a0 \u00a0 \u00a0 value = value\n\u00a0 \u00a0 }\nend\nlocal function parse_callment()\n\n\u00a0 \u00a0 local func = parse_expr()\n\u00a0 \u00a0 if not func then return end\n\n\u00a0 \u00a0 local _1 = consumeChar('(') or parsing_error(`token '(' esperado`)\n\u00a0 \u00a0 local args = { parse_expr() }\n\n\u00a0 \u00a0 while consumeChar(',') do\n\n\u00a0 \u00a0 \u00a0 \u00a0 local arg = parse_expr() or parsing_error(`argumento depois da ',' esperado`)\n\u00a0 \u00a0 \u00a0 \u00a0 table.insert(args, arg)\n\u00a0 \u00a0 end\n\u00a0 \u00a0 local _2 = consumeChar(')') or parsing_error(`token ')' esperado`)\n\n\u00a0 \u00a0 return { kind = 'callment', func = func, args = args }\nend\nlocal function parse_if(): if_stat?\n\n\u00a0 \u00a0 if not consumeWord('if') then return end\n\u00a0 \u00a0 local cond = parse_expr() or parsing_error(`expressao esperada`)\n\u00a0 \u00a0 local then_body = parse_body() or parsing_error(`instrucao esperada`)\n\u00a0 \u00a0 local else_body = if consumeWord('else') then parse_body() or parsing_error(`instrucao esperada`) else nil\n\n\u00a0 \u00a0 return { kind = 'if', cond = cond, then_body = then_body, else_body = else_body }\nend\n\nlocal function parse_stat(): stat?\n\u00a0 \u00a0 return parse_if()\n\u00a0 \u00a0 \u00a0 \u00a0 or parse_assignment() -- precedencia importante aqui\n\u00a0 \u00a0 \u00a0 \u00a0 or parse_callment()\nend\n\n-- block\nlocal function parse_block(): block?\n\n\u00a0 \u00a0 if not consumeChar('{') then return end\n\u00a0 \u00a0 local stats = {}\n\u00a0 \u00a0 repeat\n\u00a0 \u00a0 \u00a0 \u00a0 local stat = parse_stat()\n\u00a0 \u00a0 \u00a0 \u00a0 if not stat then break end\n\n\u00a0 \u00a0 \u00a0 \u00a0 table.insert(stats, stat)\n\u00a0 \u00a0 \u00a0 \u00a0 consumeChar(';')\n\u00a0 \u00a0 until false\n\n\u00a0 \u00a0 local _1 = consumeChar('}') or parsing_error(`token '}' esperado`)\n\u00a0 \u00a0 return { kind = 'block', stats = stats }\nend\n\nlocal function parse_simple_block(): block?\n\n\u00a0 \u00a0 local stat = parse_stat()\n\u00a0 \u00a0 if not stat then return end\n\n\u00a0 \u00a0 return { kind = 'block', stats = { stat } }\nend\nfunction parse_body(): block?\n\u00a0 \u00a0 return parse_block()\n\u00a0 \u00a0 \u00a0 \u00a0 or parse_simple_block()\nend\n</code></pre> Por fim, voc\u00ea pode testar rodando <pre><code>tokenizeAll([[{\n    set a to 1*2*3 + 4 + 10 + 3^5^7;\n    if a &lt; 4 {\n        print(\"a..4\")\n    } else if 6 &lt; a {\n        print(\"6..a\")\n    } else {\n        print(\"4..a..6\")\n    }\n}]])\n\nprint(tokens)\nprint('ast:', parse_block())\n</code></pre> E por hoje \u00e9 tudo, vale notar v\u00e1rios pontos a serem melhorados nesse tipo de implementa\u00e7\u00e3o.</p> <ul> <li>Primeiro de tudo, parsers s\u00e3o consideravelmente caros, e por isso, boa parte s\u00e3o implementados em linguagens de baixo n\u00edvel, para um desempenho melhor em arquivos maiores.<ul> <li>Boa parte da implementa\u00e7\u00e3o \u00e9 a forma mais eficiente, embora ainda haja algumas estrat\u00e9gias que podem ser adotadas para melhorar muito mais o desempenho</li> <li>O TokenStream pode ser transformado em uma esp\u00e9cie de lazy stream (corrente pregui\u00e7osa), onde inv\u00e9s de tentar todas as combina\u00e7\u00f5es poss\u00edveis para cada token do texto, voc\u00ea usa a pr\u00f3pria expectativa do parser para come\u00e7ar tentando \"lexar\" os Tokens corretos primeiro (que ser\u00e1 o caso mais comum onde o parser vai agir). Misturando isso com tamb\u00e9m, ordenar os sub-parsers na ordem mais estatisticamente comum, voc\u00ea pode cortar mais que o dobro de tentativas de lexing in\u00fateis. N\u00e3o s\u00f3 isso, como tentar lexar na hora de consumir te d\u00e1 resposta imediata se o token \u00e9 do tipo esperado, oque elimina para n\u00f3s uma opera\u00e7\u00e3o de leitura da propriedade \"kind\" (micro otimiza\u00e7\u00e3o de grande peso).</li> <li>Otimizar parsers recursivos com recurs\u00e3o de calda, transformando recurs\u00f5es em meros loops (desde que n\u00e3o exija criar qualquer tipo de stack (oque na pr\u00e1tica, teria o mesmo custo de recurs\u00e3o, por\u00e9m mais cara por estar sendo implementada em luau, inv\u00e9s de internamente, em C)).</li> <li>Lembrar que micro otimiza\u00e7\u00f5es v\u00e3o importar nessas pequenas fun\u00e7\u00f5es, pois alguma delas poder\u00e3o ser chamadas milhares de vezes em um \u00fanico documento</li> </ul> </li> <li>Esse parser simplesmente trava no primeiro erro encontrado, ele ainda n\u00e3o \u00e9 \"ignorante a erros\", ent\u00e3o se quiser diagn\u00f3sticos mais detalhados, voc\u00ea deve procurar implementar \"recupera\u00e7\u00e3o de erro\"</li> <li>Ele exporta muito poucas informa\u00e7\u00f5es do parsing, como localiza\u00e7\u00e3o de cada token</li> </ul> <p>Portanto, \u00e9 uma implementa\u00e7\u00e3o apenas para demonstra\u00e7\u00e3o de conceito, serve apenas para ter uma ideia de onde come\u00e7ar para fazer o seu pr\u00f3prio</p>"}]}